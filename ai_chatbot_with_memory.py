# -*- coding: utf-8 -*-
"""AI Chatbot with Memory

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o3OR4VeCzokBLLmhDBl_dThg6i_3XQrR
"""

"""
AI Chatbot with Memory (single-file demo)
-----------------------------------------
Features:
- Stores conversation memory as embeddings (sentence-transformers)
- Uses FAISS (if available) for fast vector search; falls back to brute force
- Generates replies with a local generative model (GPT-2) conditioned on:
    - system prompt
    - top-k retrieved memory snippets
    - the user's current message
- CLI chat loop with commands:
    /exit        - quit
    /memdump     - print top-100 memory entries
    /clear_mem   - clear memory DB
    /save PATH   - save memory to PATH (pickle)
    /load PATH   - load memory from PATH (pickle)
    /help        - show commands
- Simple JSON metadata per memory: {id, role, text, timestamp}

Limitations:
- GPT-2 is not optimized for long-context chat; this is a demo.
- For production you'd use a dialogue-optimized model or external LLMs.
"""

from __future__ import annotations

import argparse
import os
import pickle
import sys
import time
import uuid
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Tuple

import numpy as np

# --- Model imports with graceful fallback ---
# sentence-transformers for embeddings
try:
    from sentence_transformers import SentenceTransformer
    SBER_AVAILABLE = True
except Exception:
    SBER_AVAILABLE = False

# faiss for vector index
try:
    import faiss
    FAISS_AVAILABLE = True
except Exception:
    FAISS_AVAILABLE = False

# transformers for generation (GPT-2)
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, set_seed
    TRANSFORMERS_AVAILABLE = True
except Exception:
    TRANSFORMERS_AVAILABLE = False

# pandas for pretty printing memory dump
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except Exception:
    PANDAS_AVAILABLE = False


# --------------------------
# Data structures & helpers
# --------------------------
@dataclass
class MemoryItem:
    id: str
    role: str       # "user" or "assistant" or "system"
    text: str
    ts: float       # unix timestamp
    meta: dict


class MemoryDB:
    """In-memory memory DB with optional FAISS index for vectors."""

    def __init__(self, embed_dim: int, use_faiss: bool = True):
        self.items: List[MemoryItem] = []
        self.vectors = None  # numpy array [N, D]
        self.embed_dim = embed_dim
        self.use_faiss = use_faiss and FAISS_AVAILABLE
        self.index = None
        if self.use_faiss:
            # We'll (re)create an index on demand
            self._make_index()

    def _make_index(self):
        if not self.use_faiss:
            return
        self.index = faiss.IndexFlatIP(self.embed_dim)

    def add(self, item: MemoryItem, vector: np.ndarray):
        self.items.append(item)
        if self.vectors is None:
            self.vectors = vector.reshape(1, -1).astype("float32")
        else:
            self.vectors = np.vstack([self.vectors, vector.reshape(1, -1).astype("float32")])
        if self.use_faiss:
            # re-create index (cheap for small DB). For large DB you'd add separately.
            self._make_index()
            self.index.add(self.vectors)

    def search(self, qvec: np.ndarray, top_k: int = 5) -> List[Tuple[MemoryItem, float]]:
        """
        Return top_k (MemoryItem, score) pairs sorted by descending score (cosine via inner product on normalized vectors).
        qvec: numpy array shape (D,)
        """
        if self.vectors is None or len(self.items) == 0:
            return []

        q = qvec.reshape(1, -1).astype("float32")
        # Ensure vectors are normalized (cosine similarity via inner product)
        def normalize(a):
            a = a.astype("float32")
            norms = np.linalg.norm(a, axis=1, keepdims=True)
            norms[norms == 0] = 1.0
            return a / norms

        qn = normalize(q)
        vn = normalize(self.vectors)

        if self.use_faiss and self.index is not None:
            # Faiss doesn't store normalized vectors here — we will create a local index on normalized vectors
            idx = faiss.IndexFlatIP(self.embed_dim)
            idx.add(vn)
            D, I = idx.search(qn, min(top_k, len(self.items)))
            scores = D[0].tolist()
            inds = I[0].tolist()
            results = []
            for i, s in zip(inds, scores):
                if i < 0:
                    continue
                results.append((self.items[i], float(s)))
            return results
        else:
            # brute force
            qn = qn[0]
            scores = (vn @ qn).tolist()
            idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
            return [(self.items[i], float(scores[i])) for i in idxs]

    def dump(self, limit: Optional[int] = 100):
        rows = []
        for it in self.items[-limit:]:
            rows.append({"id": it.id, "role": it.role, "text": it.text, "ts": datetime.utcfromtimestamp(it.ts).isoformat()})
        return rows

    def clear(self):
        self.items = []
        self.vectors = None
        if self.use_faiss:
            self._make_index()

    def save(self, path: str):
        with open(path, "wb") as f:
            pickle.dump({"items": self.items, "vectors": self.vectors, "embed_dim": self.embed_dim}, f)

    def load(self, path: str):
        with open(path, "rb") as f:
            data = pickle.load(f)
        self.items = data.get("items", [])
        self.vectors = data.get("vectors", None)
        self.embed_dim = data.get("embed_dim", self.embed_dim)
        if self.use_faiss:
            self._make_index()
            if self.vectors is not None:
                self.index.add(self.vectors)


# --------------------------
# Embedding & generator
# --------------------------

class Embedder:
    """Wrap sentence-transformers model or a simple fallback (TF-IDF-like)."""
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        if SBER_AVAILABLE:
            try:
                self.model = SentenceTransformer(model_name)
                self.dim = self.model.get_sentence_embedding_dimension()
                self._encode = self._sbert_encode
            except Exception as e:
                print("[WARN] Failed to load SentenceTransformer; falling back to simple encoder:", e)
                SBER_AVAILABLE_LOCAL = False
                self.model = None
                self.dim = 384
                self._encode = self._simple_encode
        else:
            print("[WARN] sentence-transformers not installed. Falling back to simple encoder (less accurate).")
            self.model = None
            self.dim = 384
            self._encode = self._simple_encode

    def _sbert_encode(self, texts: List[str]) -> np.ndarray:
        arr = self.model.encode(texts, show_progress_bar=False, convert_to_numpy=True)
        return arr

    def _simple_encode(self, texts: List[str]) -> np.ndarray:
        # Very naive: hash tokens into fixed dim vector
        out = np.zeros((len(texts), self.dim), dtype="float32")
        for i, t in enumerate(texts):
            h = 0
            for ch in t:
                h = (h * 31 + ord(ch)) % (2 ** 32)
            rng = np.random.RandomState(h)
            out[i] = rng.normal(size=(self.dim,))
        return out

    def encode(self, texts: List[str]) -> np.ndarray:
        # returns shape (N, dim)
        return self._encode(texts)


class Generator:
    """Wrap a local causal LM for generation. Uses transformers pipeline if available."""
    def __init__(self, model_name: str = "gpt2", device: int = -1, seed: int = 42):
        if not TRANSFORMERS_AVAILABLE:
            print("[WARN] transformers not installed — generation is disabled.")
            self.pipe = None
            self.tokenizer = None
            self.model = None
            self.available = False
            return

        # Try to load model/tokenizer
        try:
            # small model by default
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForCausalLM.from_pretrained(model_name)
            # pipeline handles device mapping; on CPU device=-1
            self.pipe = pipeline("text-generation", model=self.model, tokenizer=self.tokenizer, device=device)
            set_seed(seed)
            self.available = True
        except Exception as e:
            print("[WARN] Failed to load generation model:", e)
            self.pipe = None
            self.available = False

    def generate(self, prompt: str, max_new_tokens: int = 128, temperature: float = 0.7, top_k: int = 50) -> str:
        if not self.available:
            # fallback: echo with prefix
            return "Sorry — generation unavailable. You said: " + prompt[:200]
        out = self.pipe(prompt, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k, do_sample=True)
        text = out[0]["generated_text"]
        # remove the prompt prefix
        if text.startswith(prompt):
            text = text[len(prompt):]
        return text.strip()


# --------------------------
# Chatbot logic
# --------------------------

class Chatbot:
    def __init__(self,
                 system_prompt: str = "You are a helpful assistant.",
                 embed_model: str = "all-MiniLM-L6-v2",
                 gen_model: str = "gpt2",
                 use_faiss: bool = True,
                 top_k: int = 5):
        self.system_prompt = system_prompt
        self.embedder = Embedder(embed_model)
        self.gen = Generator(gen_model)
        self.mem = MemoryDB(embed_dim=self.embedder.dim, use_faiss=use_faiss)
        self.top_k = top_k
        # Add system prompt to memory
        self._add_memory("system", system_prompt, {"note": "system prompt"})

    def _add_memory(self, role: str, text: str, meta: dict = None):
        meta = meta or {}
        item = MemoryItem(id=str(uuid.uuid4()), role=role, text=text, ts=time.time(), meta=meta)
        vec = self.embedder.encode([text])[0]
        self.mem.add(item, vec)

    def remember_user(self, text: str):
        self._add_memory("user", text, {"source": "chat"})

    def remember_assistant(self, text: str):
        self._add_memory("assistant", text, {"source": "chat"})

    def retrieve_relevant(self, text: str, top_k: Optional[int] = None) -> List[Tuple[MemoryItem, float]]:
        top_k = top_k or self.top_k
        qvec = self.embedder.encode([text])[0]
        hits = self.mem.search(qvec, top_k=top_k)
        return hits

    def build_prompt(self, user_text: str, retrieved: List[Tuple[MemoryItem, float]]) -> str:
        # Build a prompt for the generator: include system prompt, some top memories, and user message.
        pieces = []
        pieces.append(f"System: {self.system_prompt}\n")
        if retrieved:
            pieces.append("Relevant memories (short):\n")
            for it, score in retrieved:
                ts = datetime.utcfromtimestamp(it.ts).strftime("%Y-%m-%d %H:%M:%S")
                pieces.append(f"- ({it.role}) {it.text} [score={score:.3f}]\n")
            pieces.append("\n")
        pieces.append("Conversation:\n")
        # Append last few messages (we keep just user + assistant turn)
        pieces.append(f"User: {user_text}\nAssistant:")
        return "\n".join(pieces)

    def respond(self, user_text: str) -> str:
        # Remember user utterance
        self.remember_user(user_text)
        # Retrieve
        hits = self.retrieve_relevant(user_text, top_k=self.top_k)
        # Build prompt & generate
        prompt = self.build_prompt(user_text, hits)
        generated = self.gen.generate(prompt, max_new_tokens=128)
        # Keep only first paragraph / stop at newline patterns to avoid run-on
        reply = generated.split("\n\n")[0].strip()
        # Remember assistant reply
        self.remember_assistant(reply)
        return reply

    # Utility methods for CLI
    def mem_dump(self, limit: int = 100):
        return self.mem.dump(limit)

    def mem_clear(self):
        self.mem.clear()

    def mem_save(self, path: str):
        self.mem.save(path)

    def mem_load(self, path: str):
        self.mem.load(path)


# --------------------------
# CLI loop
# --------------------------

def repl(chatbot: Chatbot):
    print("=== AI Chatbot with Memory ===")
    print("Type your message and press Enter. Use /help for commands.")
    while True:
        try:
            user = input("\nYou: ").strip()
        except (KeyboardInterrupt, EOFError):
            print("\nGoodbye.")
            break
        if not user:
            continue
        if user.startswith("/"):
            parts = user.split(maxsplit=2)
            cmd = parts[0].lower()
            if cmd == "/exit":
                print("Bye.")
                break
            elif cmd == "/help":
                print("Commands: /exit, /help, /memdump, /clear_mem, /save PATH, /load PATH")
                continue
            elif cmd == "/memdump":
                rows = chatbot.mem_dump(200)
                if PANDAS_AVAILABLE:
                    df = pd.DataFrame(rows)
                    print(df.to_string(index=False))
                else:
                    for r in rows:
                        print(f"[{r['ts']}] ({r['role']}) {r['text']}")
                continue
            elif cmd == "/clear_mem":
                chatbot.mem_clear()
                print("Memory cleared.")
                continue
            elif cmd == "/save":
                if len(parts) < 2:
                    print("Usage: /save path")
                    continue
                path = parts[1]
                chatbot.mem_save(path)
                print(f"Saved memory -> {path}")
                continue
            elif cmd == "/load":
                if len(parts) < 2:
                    print("Usage: /load path")
                    continue
                path = parts[1]
                chatbot.mem_load(path)
                print(f"Loaded memory <- {path}")
                continue
            else:
                print("Unknown command. Use /help.")
                continue
        # Normal message -> generate reply
        reply = chatbot.respond(user)
        print(f"\nAssistant: {reply}")


# --------------------------
# Entrypoint
# --------------------------

def main():
    p = argparse.ArgumentParser(prog="chatbot_with_memory.py", description="AI Chatbot with Memory (demo)")
    p.add_argument("--system", default="You are a helpful assistant.", help="System prompt")
    p.add_argument("--embed-model", default="all-MiniLM-L6-v2", help="SentenceTransformer model name")
    p.add_argument("--gen-model", default="gpt2", help="Generative model (transformers) name")
    p.add_argument("--no-faiss", action="store_true", help="Disable faiss even if installed")
    p.add_argument("--top-k", type=int, default=5, help="How many memories to retrieve")
    args = p.parse_args()

    chatbot = Chatbot(system_prompt=args.system,
                      embed_model=args.embed_model,
                      gen_model=args.gen_model,
                      use_faiss=not args.no_faiss,
                      top_k=args.top_k)
    repl(chatbot)


if __name__ == "__main__":
    main()